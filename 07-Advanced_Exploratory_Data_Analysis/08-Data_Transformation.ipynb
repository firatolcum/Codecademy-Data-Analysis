{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a823fff",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a21ea",
   "metadata": {},
   "source": [
    "## 1. Data Centering and Scaling\n",
    "#### 1. Data Centering\n",
    "- Data centering involves subtracting the mean of a data set from each data point so that the new mean is 0. Mathematically, this looks like:\n",
    "    - Xcentered_i= X_i - μ (where X_i is a datapoint and the Greek letter μ is the mean of all the X values.)\n",
    "- This centered data is useful because it tells us how far above or below the mean each data point is, giving us additional insight that we can’t get just by looking at the initial data set. \n",
    "\n",
    "#### 2. Data Scaling\n",
    "- A common task for data analysts and scientists is to find trends in data by comparing features of data points. However, this task is made difficult when the features are on drastically different scales.\n",
    "- For instance, let’s consider a data set containing two features, age and income. In general, a person’s age usually ranges from 0 to about 100 years. A person’s income, on the other hand, usually ranges from 0 to large amounts measured in the thousands of dollars. Clearly, age and income are two features that have vastly different ranges.\n",
    "- This presents issues when trying to use many machine learning algorithms, which treat all dimensions equally regardless of their scale. The difference in one year of age is interpreted as exactly equal to the difference in one dollar of income. That makes no sense! In other words, the income feature outweighs the importance of age because income is on a relatively huge scale. \n",
    "\n",
    "\n",
    " -   We would like every datapoint to have the same scale so each feature contributes equally to the relationship. Data scaling lets us achieve this.\n",
    "\n",
    " -  Two of the most commonly used data scaling techniques are Min-max normalization and Standardization.\n",
    "     1. Min-max normalization:\n",
    "         - Min-max normalization is one of the most simple and common ways to scale data.\n",
    "         - For every feature in a data set, the minimum value of that feature is transformed into 0, the maximum value is transformed into 1, and every other value is transformed into a decimal between 0 and 1.\n",
    "         - One downside of min-max normalization is that it does not handle outliers very well. \n",
    "         - Standardization is a more robust data scaling method for dealing with outliers.\n",
    "     2. Standardization\n",
    "         - Standardization (also known as Z-score normalization) is another common data scaling technique.\n",
    "         - Standardization involves subtracting the mean of each observation and then dividing by the standard deviation.\n",
    "             - (z = (value - mean) / stdev)\n",
    "         - Once standardization is complete, all the features will have a mean of zero, a standard deviation of one, and therefore, the same scale.\n",
    "         - Unlike normalization, standardization does not have a bounding range. This means that even if you have outliers in your data, your standardized data will not be affected. Therefore, if your dataset has outliers, standardization is the preferred scaling technique.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a126e258",
   "metadata": {},
   "source": [
    "### Example of Min-Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8ff95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(lst):\n",
    "    minimum = min(lst)\n",
    "    maximum = max(lst)\n",
    "    normalized = []\n",
    "    for i in lst:\n",
    "        normalized.append((i - min(lst)) / (max(lst) - min(lst)))\n",
    "  # code goes here\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d97f9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.25, 0.5, 0.75, 1.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_normalize([0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91568337",
   "metadata": {},
   "source": [
    "### Example of Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112bfb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(lst, mean, std_dev):\n",
    "    standardized = []\n",
    "    for i in lst:\n",
    "        standardized.append((i - mean) / std_dev)\n",
    "  # code goes here\n",
    "\n",
    "    return standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ff726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.4184397163120568,\n",
       " -0.7092198581560284,\n",
       " 0.0,\n",
       " 0.7092198581560284,\n",
       " 1.4184397163120568]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardize([1, 2, 3, 4, 5], 3.0, 1.41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9361cad",
   "metadata": {},
   "source": [
    "### When to Normalize vs. Standardize?\n",
    "Min-max normalization and standardization both have a similar goal of transforming features in data to have the same scale so that each feature is equally important. So when should you use min-max normalization vs. standardization?\n",
    "\n",
    "There is not always a clear answer. Both normalization and standardization have their strengths as well as their drawbacks. For example, if you need your data to be on a 0-1 scale, then it makes sense to use min-max normalization. If you have outliers in your data, then it is best to use standardization (Z-score normalization) since it does not have a bounding range like min-max normalization does.\n",
    "\n",
    "Keep in mind that not every data set requires normalization or standardization. If your data features do not have vastly different ranges, then scaling your data might not be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348e654",
   "metadata": {},
   "source": [
    "## 2. Discretizing Numerical Data and Collapsing Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271efe7f",
   "metadata": {},
   "source": [
    "###  Binning Data\n",
    "As a data analyst or scientist, you will sometimes deal with numerical data that you would like to arrange into different categories. For example, imagine you are looking at the ages of a set of individuals. You might care more about the age category each person falls under (20-29, 30-39, 40-49, etc) than the exact age. The process of transforming numerical variables into categorical counterparts is called “binning.”\n",
    "\n",
    "Binning is a way to group a number of continuous values into a smaller number of “bins”. We see this in the real world quite often. For example, a student’s letter grade is determined by the percentage “bin” they fall under. Moreover, an individual’s tax rate is determined by the income “bin” they fall under.\n",
    "\n",
    "### Why bin data?\n",
    "Binning data can be useful in many situations, whether it be to create more appealing visualizations or to improve a machine learning model.\n",
    "\n",
    "In areas like machine learning, binning data can be helpful to improve the accuracy of predictive models by reducing the noise of the data. For example, if you’re a machine learning engineer working at an investment firm, you might use binning as a technique to improve your stock prediction models by smoothing the data to reduce the impact of small, short term price changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78132ba",
   "metadata": {},
   "source": [
    "## Advanced Data Transformations\n",
    "An introduction to log transformation and other advanced data transformations\n",
    "\n",
    "In this article, you will learn how to transform data in particular situations, such as when the data you’re working with is skewed. You will learn:\n",
    "\n",
    "- How to transform data that is skewed\n",
    "- What is log transformation and why is it useful?\n",
    "- How to implement log transformation in Python\n",
    "- Different kinds of advanced data transformations\n",
    "\n",
    "### Skewed Data\n",
    "As a data analyst and scientist, you will often work with normally distributed data. Just as a refresher, normally distributed data is data where most data points are close to a central value with fewer instances as you get further away from the center. Visually, this looks like a “bell curve”.\n",
    "\n",
    "There are several reasons why having normally distributed data is useful:\n",
    "- Normal distributions are symmetric around their mean. Moreover, the mean, median, and mode of a normal distribution are equal. These properties make the data easier to analyze because we can make some assumptions about how many datapoints are on either side of the mean.\n",
    "-  Approximately 68% of the data falls within 1 standard deviation of the mean, and approximately 95% of the data falls within 2 standard deviations of the mean. Again, these properties make it easier to make generalizations about the data you’re working with.\n",
    "- Many machine learning algorithms (such as linear regression) assume that the data distribution is close to normal. Therefore, transforming data so that it is normally distributed can enhance your ability to fit an accurate predictive model.\n",
    "\n",
    "Oftentimes, real-life data is messy and does not conform to a normal distribution. Instead, the data might be skewed to the left or right.\n",
    "- For example, imagine you’re a data analyst looking at income data in a population. You may find that a lot of the data is centered around lower values but that there are high values in the data as well. This results in a skewed distribution.\n",
    "- Feeding this data to some machine learning or statistical models will present issues if the model assumes normality, because the model will be trained on a much larger number of lower incomes. The skewness of the data violates the assumptions of the model.\n",
    "- In situations like this, we would like to make our data less skewed so that it conforms to model assumptions and is easier to work with. To help with this issue, we can use Log Transformation.\n",
    "\n",
    "Generally, if skewness is less than -1 or greater than 1, the distribution is highly skewed. \n",
    "\n",
    "### Log Transformation\n",
    "\n",
    "Log transformation is a data transformation method that replaces each variable x with log(x).\n",
    "\n",
    "When log transformation is applied to data that is not normally distributed, the result is that the data will be less skewed, or more “normal” than before.\n",
    "    \n",
    "#### Log Transformation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "095d01bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
       "       0.69314718, 0.69314718, 1.09861229, 1.09861229, 1.38629436,\n",
       "       1.38629436])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4]\n",
    "\n",
    "log_data = np.log(data)\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2b133e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "log_transform = PowerTransformer()\n",
    "log_transform.fit_transform([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90719e",
   "metadata": {},
   "source": [
    "### Other Transformations\n",
    "What happens if you have negative numbers in your data? You cannot take the log of a negative number, so log transformation will not work. In this case, you would have to explore other data transformation techniques, such as cube root transformation, which involves converting x to x^(1/3). This transformation reduces the right skewness but also has the benefit of working with zero and negative values.\n",
    "\n",
    "To reduce left skewness, you might look at a technique such as square transformation, which involves converting x to x^2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa5d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
